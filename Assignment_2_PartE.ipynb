{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15ca839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing decision tree from above written method for depth 1\n",
      "TREE\n",
      "+-- [SPLIT: x4 = 0 True]\n",
      "|\t+-- [LABEL = 0]\n",
      "+-- [SPLIT: x4 = 0 False]\n",
      "|\t+-- [LABEL = 1]\n",
      "Test Error = 23.29%.\n",
      "Printing decision tree by scikitlearns decision tree classifier for depth 1\n",
      "|--- feature_4 <= 0.50\n",
      "|   |--- class: 0\n",
      "|--- feature_4 >  0.50\n",
      "|   |--- class: 1\n",
      "\n",
      "Test Error = 23.29%.\n",
      "Printing decision tree from above written method for depth 3\n",
      "TREE\n",
      "+-- [SPLIT: x4 = 0 True]\n",
      "|\t+-- [SPLIT: x3 = 10 True]\n",
      "|\t|\t+-- [SPLIT: x7 = 0 True]\n",
      "|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t+-- [SPLIT: x7 = 0 False]\n",
      "|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t+-- [SPLIT: x3 = 10 False]\n",
      "|\t|\t+-- [SPLIT: x1 = 3 True]\n",
      "|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t+-- [SPLIT: x1 = 3 False]\n",
      "|\t|\t|\t+-- [LABEL = 0]\n",
      "+-- [SPLIT: x4 = 0 False]\n",
      "|\t+-- [SPLIT: x7 = 1 True]\n",
      "|\t|\t+-- [SPLIT: x0 = 0 True]\n",
      "|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t|\t+-- [SPLIT: x0 = 0 False]\n",
      "|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t+-- [SPLIT: x7 = 1 False]\n",
      "|\t|\t+-- [SPLIT: x1 = 2 True]\n",
      "|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t+-- [SPLIT: x1 = 2 False]\n",
      "|\t|\t|\t+-- [LABEL = 1]\n",
      "Test Error = 28.77%.\n",
      "Printing decision tree by scikitlearns decision tree classifier for depth 3\n",
      "|--- feature_4 <= 0.50\n",
      "|   |--- feature_3 <= 7.50\n",
      "|   |   |--- feature_6 <= 0.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |   |--- feature_6 >  0.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |--- feature_3 >  7.50\n",
      "|   |   |--- feature_8 <= 3.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature_8 >  3.50\n",
      "|   |   |   |--- class: 0\n",
      "|--- feature_4 >  0.50\n",
      "|   |--- feature_7 <= 0.50\n",
      "|   |   |--- feature_3 <= 4.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature_3 >  4.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |--- feature_7 >  0.50\n",
      "|   |   |--- feature_0 <= 0.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature_0 >  0.50\n",
      "|   |   |   |--- class: 0\n",
      "\n",
      "Test Error = 28.77%.\n",
      "Printing decision tree from above written method for depth 5\n",
      "TREE\n",
      "+-- [SPLIT: x4 = 0 True]\n",
      "|\t+-- [SPLIT: x3 = 10 True]\n",
      "|\t|\t+-- [SPLIT: x7 = 0 True]\n",
      "|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t+-- [SPLIT: x7 = 0 False]\n",
      "|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t+-- [SPLIT: x3 = 10 False]\n",
      "|\t|\t+-- [SPLIT: x1 = 3 True]\n",
      "|\t|\t|\t+-- [SPLIT: x5 = 2 True]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x3 = 2 True]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x3 = 2 False]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t|\t+-- [SPLIT: x5 = 2 False]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x0 = 0 True]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x0 = 0 False]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t+-- [SPLIT: x1 = 3 False]\n",
      "|\t|\t|\t+-- [SPLIT: x3 = 8 True]\n",
      "|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t|\t|\t+-- [SPLIT: x3 = 8 False]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x5 = 0 True]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x5 = 0 False]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "+-- [SPLIT: x4 = 0 False]\n",
      "|\t+-- [SPLIT: x7 = 1 True]\n",
      "|\t|\t+-- [SPLIT: x0 = 1 True]\n",
      "|\t|\t|\t+-- [SPLIT: x1 = 1 True]\n",
      "|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t|\t|\t+-- [SPLIT: x1 = 1 False]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x3 = 3 True]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x3 = 3 False]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t+-- [SPLIT: x0 = 1 False]\n",
      "|\t|\t|\t+-- [SPLIT: x3 = 5 True]\n",
      "|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t|\t+-- [SPLIT: x3 = 5 False]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x3 = 7 True]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x3 = 7 False]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t+-- [SPLIT: x7 = 1 False]\n",
      "|\t|\t+-- [SPLIT: x1 = 2 True]\n",
      "|\t|\t|\t+-- [SPLIT: x4 = 1 True]\n",
      "|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t|\t|\t+-- [SPLIT: x4 = 1 False]\n",
      "|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t+-- [SPLIT: x1 = 2 False]\n",
      "|\t|\t|\t+-- [SPLIT: x3 = 6 True]\n",
      "|\t|\t|\t|\t+-- [LABEL = 0]\n",
      "|\t|\t|\t+-- [SPLIT: x3 = 6 False]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x3 = 4 True]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "|\t|\t|\t|\t+-- [SPLIT: x3 = 4 False]\n",
      "|\t|\t|\t|\t|\t+-- [LABEL = 1]\n",
      "Test Error = 20.55%.\n",
      "Printing decision tree by scikitlearns decision tree classifier for depth 5\n",
      "|--- feature_4 <= 0.50\n",
      "|   |--- feature_3 <= 7.50\n",
      "|   |   |--- feature_6 <= 0.50\n",
      "|   |   |   |--- feature_1 <= 2.50\n",
      "|   |   |   |   |--- feature_8 <= 2.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_8 >  2.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- feature_1 >  2.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |--- feature_6 >  0.50\n",
      "|   |   |   |--- feature_5 <= 0.50\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_5 >  0.50\n",
      "|   |   |   |   |--- feature_5 <= 1.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_5 >  1.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |--- feature_3 >  7.50\n",
      "|   |   |--- feature_8 <= 3.50\n",
      "|   |   |   |--- feature_2 <= 1.00\n",
      "|   |   |   |   |--- feature_8 <= 2.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_8 >  2.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_2 >  1.00\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature_8 >  3.50\n",
      "|   |   |   |--- class: 0\n",
      "|--- feature_4 >  0.50\n",
      "|   |--- feature_7 <= 0.50\n",
      "|   |   |--- feature_3 <= 4.50\n",
      "|   |   |   |--- feature_0 <= 0.50\n",
      "|   |   |   |   |--- feature_3 <= 3.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_3 >  3.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_0 >  0.50\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature_3 >  4.50\n",
      "|   |   |   |--- feature_4 <= 5.50\n",
      "|   |   |   |   |--- feature_3 <= 6.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_3 >  6.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_4 >  5.50\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |--- feature_7 >  0.50\n",
      "|   |   |--- feature_0 <= 0.50\n",
      "|   |   |   |--- feature_3 <= 5.50\n",
      "|   |   |   |   |--- feature_3 <= 4.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_3 >  4.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- feature_3 >  5.50\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature_0 >  0.50\n",
      "|   |   |   |--- feature_1 <= 1.50\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_1 >  1.50\n",
      "|   |   |   |   |--- feature_3 <= 3.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_3 >  3.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "\n",
      "Test Error = 20.55%.\n",
      "Confusion Matrix for depth 1 for decision tree using our own method\n",
      "[[47 12]\n",
      " [ 5  9]]\n",
      "\n",
      "\n",
      "Confusion Matrix for depth 1 for decision tree using scikit learn\n",
      "[[47 12]\n",
      " [ 5  9]]\n",
      "\n",
      "\n",
      "Confusion Matrix for depth 3 for decision tree using our own method\n",
      "[[47 12]\n",
      " [ 9  5]]\n",
      "\n",
      "\n",
      "Confusion Matrix for depth 3 for decision tree using scikit learn\n",
      "[[45 14]\n",
      " [ 7  7]]\n",
      "\n",
      "\n",
      "Confusion Matrix for depth 5 for decision tree using our own method\n",
      "[[50  9]\n",
      " [ 6  8]]\n",
      "\n",
      "\n",
      "Confusion Matrix for depth 5 for decision tree using scikit learn\n",
      "[[50  9]\n",
      " [ 6  8]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################################################################\n",
    "# CS6375.001 Machine Learning - Assignment 2 - Decision Tree\n",
    "# Name: Karneeshwar Sendilkumar Vijaya\n",
    "# NetID: KXS200001\n",
    "###############################################################################################################################################\n",
    "\n",
    "# Part-E Solution, using SPECT data\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import graphviz\n",
    "import math\n",
    "\n",
    "def partition(x):\n",
    "    \"\"\"\n",
    "    Partition the column vector x into subsets indexed by its unique values_data (v1, ... vk)\n",
    "\n",
    "    Returns a dictionary of the form\n",
    "    { v1: indices of x == v1,\n",
    "      v2: indices of x == v2,\n",
    "      ...\n",
    "      vk: indices of x == vk }, where [v1, ... vk] are all the unique values_data in the vector z.\n",
    "    \"\"\"\n",
    "    parts = {}\n",
    "    i = 0\n",
    "    \n",
    "    for j in x:\n",
    "        if j in parts:\n",
    "            parts[j].append(i)\n",
    "        else:\n",
    "            parts[j] = [i]\n",
    "        i += 1\n",
    "    return parts\n",
    "\n",
    "    raise Exception('Function not yet implemented!')\n",
    "    \n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Compute the entropy of a vector y by considering the counts of the unique values_data (v1, ... vk), in z\n",
    "\n",
    "    Returns the entropy of z: H(z) = p(z=v1) log2(p(z=v1)) + ... + p(z=vk) log2(p(z=vk))\n",
    "    \"\"\"\n",
    "\n",
    "    ent=0;\n",
    "    valuesy,county = np.unique(y,return_counts = True)\n",
    "    \n",
    "    for z in county:\n",
    "        ent = ent + ((z/len(y))*(math.log(z/len(y),2)))\n",
    "        \n",
    "    return -ent\n",
    "\n",
    "    raise Exception('Function not yet implemented!')\n",
    "\n",
    "\n",
    "def mutual_information(x, y):\n",
    "    \"\"\"\n",
    "    Compute the mutual information between a data column (x) and the labels (y). The data column is a single attribute\n",
    "    over all the examples (n x 1). Mutual information is the difference between the entropy BEFORE the split set, and\n",
    "    the weighted-average entropy of EACH possible split.\n",
    "\n",
    "    Returns the mutual information: I(x, y) = H(y) - H(y | x)\n",
    "    \"\"\"\n",
    "\n",
    "    Mutual_Information = {}\n",
    "    values_data, counts = np.unique(x, return_counts=True) \n",
    "    Entropy_y = entropy(y)\n",
    "\n",
    "    for v in values_data:  \n",
    "        ny1 = y[np.where(x == v)]\n",
    "        ny2 = y[np.where(x != v)]\n",
    "        Mutual_Information[v] = (Entropy_y - ((len(ny1)/len(y)*entropy(ny1)) + (len(ny2)/len(y)*entropy(ny2))))\n",
    "    return Mutual_Information\n",
    "\n",
    "    raise Exception('Function not yet implemented!')\n",
    "\n",
    "\n",
    "def id3(x, y, attribute_value_pairs=None, depth=0, max_depth=5):\n",
    "    \"\"\"\n",
    "    Implements the classical ID3 algorithm given training data (x), training labels (y) and an array of\n",
    "    attribute-value pairs to consider. This is a recursive algorithm that depends on three termination conditions\n",
    "        1. If the entire set of labels (y) is pure (all y = only 0 or only 1), then return that label\n",
    "        2. If the set of attribute-value pairs is empty (there is nothing to split on), then return the most common\n",
    "           value of y (majority label)\n",
    "        3. If the max_depth is reached (pre-pruning bias), then return the most common value of y (majority label)\n",
    "    Otherwise the algorithm selects the next best attribute-value pair using INFORMATION GAIN as the splitting criterion\n",
    "    and partitions the data set based on the values_data of that attribute before the next recursive call to ID3.\n",
    "\n",
    "    The tree we learn is a BINARY tree, which means that every node has only two branches. The splitting criterion has\n",
    "    to be chosen from among all possible attribute-value pairs. That is, for a problem with two features/attributes x1\n",
    "    (taking values_data a, b, c) and x2 (taking values_data d, e), the initial attribute value pair list is a list of all pairs of\n",
    "    attributes with their corresponding values_data:\n",
    "    [(x1, a),\n",
    "     (x1, b),\n",
    "     (x1, c),\n",
    "     (x2, d),\n",
    "     (x2, e)]\n",
    "     If we select (x2, d) as the best attribute-value pair, then the new decision node becomes: [ (x2 == d)? ] and\n",
    "     the attribute-value pair (x2, d) is removed from the list of attribute_value_pairs.\n",
    "\n",
    "    The tree is stored as a nested dictionary, where each entry is of the form\n",
    "                    (attribute_index, attribute_value, True/False): subtree\n",
    "    * The (attribute_index, attribute_value) determines the splitting criterion of the current node. For example, (4, 2)\n",
    "    indicates that we test if (x4 == 2) at the current node.\n",
    "    * The subtree itself can be nested dictionary, or a single label (leaf node).\n",
    "    * Leaf nodes are (majority) class labels\n",
    "\n",
    "    Returns a decision tree represented as a nested dictionary, for example\n",
    "    {(4, 1, False):\n",
    "        {(0, 1, False):\n",
    "            {(1, 1, False): 1,\n",
    "             (1, 1, True): 0},\n",
    "         (0, 1, True):\n",
    "            {(1, 1, False): 0,\n",
    "             (1, 1, True): 1}},\n",
    "     (4, 1, True): 1}\n",
    "    \"\"\"\n",
    "    values_data = np.unique(y)\n",
    "    \n",
    "    if len(values_data) == 1: \n",
    "        return values_data[0]\n",
    "    \n",
    "    if attribute_value_pairs == 0:\n",
    "        values_data, counts = np.unique(y, return_counts=True)\n",
    "        return values_data[np.argmax(counts)]\n",
    "    \n",
    "    if depth == max_depth:\n",
    "        values_data, counts = np.unique(y, return_counts=True)\n",
    "        return values_data[np.argmax(counts)]\n",
    "    \n",
    "    decision_tree = {}\n",
    "    gain_max_att = 0\n",
    "    max_att = 0\n",
    "    max_att_value = 0\n",
    "    for att, att_value in attribute_value_pairs:\n",
    "        MI = mutual_information(x[:,att], y)\n",
    "        gain_att_value = MI.get(att_value)\n",
    "        if gain_att_value == None:\n",
    "            continue\n",
    "            \n",
    "        max_gain_MI = gain_att_value\n",
    "        max_value_MI = att_value\n",
    "        if ((max_gain_MI > gain_max_att)):\n",
    "            gain_max_att = max_gain_MI \n",
    "            max_att = att\n",
    "            max_att_value = max_value_MI\n",
    "    \n",
    "    if((gain_max_att != 0) or (max_att != 0) or (max_att_value != 0)):\n",
    "        attribute_value_pairs.remove((max_att, max_att_value))\n",
    "        true_side = (max_att, max_att_value, True)\n",
    "        false_side = (max_att, max_att_value, False)\n",
    "        partition_x = partition(x[:,max_att])\n",
    "        x_att_indices = partition_x[max_att_value]\n",
    "        false_y = y[np.where(x[: ,max_att] != max_att_value)]\n",
    "        false_x = x[np.where(x[: ,max_att] != max_att_value)]\n",
    "        a = id3(x[x_att_indices], y[x_att_indices], attribute_value_pairs, depth+1, max_depth)\n",
    "        b = id3(false_x, false_y, attribute_value_pairs, depth+1, max_depth)\n",
    "        decision_tree = {true_side: a, false_side: b}\n",
    "    else:\n",
    "        values_data, counts = np.unique(y, return_counts = True)\n",
    "        return values_data[np.argmax(counts)]\n",
    "    return decision_tree\n",
    "    \n",
    "    raise Exception('Function not yet implemented!')\n",
    "\n",
    "\n",
    "def predict_example(x, tree):\n",
    "    \"\"\"\n",
    "    Predicts the classification label for a single example x using tree by recursively descending the tree until\n",
    "    a label/leaf node is reached.\n",
    "\n",
    "    Returns the predicted label of x according to tree\n",
    "    \"\"\"\n",
    "\n",
    "    for split, subtree in tree.items():\n",
    "        att_num = split[0]\n",
    "        att_value = split[1]\n",
    "        d = split[2]\n",
    "\n",
    "        if d == (x[att_num] == att_value):\n",
    "            if type(subtree) is dict:\n",
    "                return predict_example(x, subtree)\n",
    "            else:\n",
    "                return subtree\n",
    "            \n",
    "    raise Exception('Function not yet implemented!')\n",
    "\n",
    "\n",
    "def compute_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the average error between the true labels (y_true) and the predicted labels (y_pred)\n",
    "\n",
    "    Returns the error = (1/n) * sum(y_true != y_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    length = len(y_true)\n",
    "    err = [y_true[i] != y_pred[i] for i in range(length)]\n",
    "    return sum(err)/length\n",
    "\n",
    "    raise Exception('Function not yet implemented!')\n",
    "\n",
    "\n",
    "def pretty_print(tree, depth=0):\n",
    "    \"\"\"\n",
    "    Pretty prints the decision tree to the console. Use print(tree) to print the raw nested dictionary representation\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        print('TREE')\n",
    "\n",
    "    for index, split_criterion in enumerate(tree):\n",
    "        sub_trees = tree[split_criterion]\n",
    "\n",
    "        # Print the current node: split criterion\n",
    "        print('|\\t' * depth, end='')\n",
    "        print('+-- [SPLIT: x{0} = {1} {2}]'.format(split_criterion[0], split_criterion[1], split_criterion[2]))\n",
    "\n",
    "        # Print the children\n",
    "        if type(sub_trees) is dict:\n",
    "            pretty_print(sub_trees, depth + 1)\n",
    "        else:\n",
    "            print('|\\t' * (depth + 1), end='')\n",
    "            print('+-- [LABEL = {0}]'.format(sub_trees))\n",
    "\n",
    "\n",
    "def render_dot_file(dot_string, save_file, image_format='png'):\n",
    "    \"\"\"\n",
    "    Uses GraphViz to render a dot file. The dot file can be generated using\n",
    "        * sklearn.tree.export_graphviz()' for decision trees produced by scikit-learn\n",
    "        * to_graphviz() (function is in this file) for decision trees produced by  your code.\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "    if type(dot_string).__name__ != 'str':\n",
    "        raise TypeError('visualize() requires a string representation of a decision tree.\\nUse tree.export_graphviz()'\n",
    "                        'for decision trees produced by scikit-learn and to_graphviz() for decision trees produced by'\n",
    "                        'your code.\\n')\n",
    "\n",
    "    # Set path to your GraphViz executable here\n",
    "    os.environ[\"PATH\"] += os.pathsep + '/opt/homebrew/Cellar/graphviz/2.50.0/bin/'\n",
    "    graph = graphviz.Source(dot_string)\n",
    "    graph.format = image_format\n",
    "    graph.render(save_file, view=True)\n",
    "\n",
    "\n",
    "def to_graphviz(tree, dot_string='', uid=-1, depth=0):\n",
    "    \"\"\"\n",
    "    Converts a tree to DOT format for use with visualize/GraphViz\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "\n",
    "    uid += 1       # Running index of node ids across recursion\n",
    "    node_id = uid  # Node id of this node\n",
    "\n",
    "    if depth == 0:\n",
    "        dot_string += 'digraph TREE {\\n'\n",
    "\n",
    "    for split_criterion in tree:\n",
    "        sub_trees = tree[split_criterion]\n",
    "        attribute_index = split_criterion[0]\n",
    "        attribute_value = split_criterion[1]\n",
    "        split_decision = split_criterion[2]\n",
    "\n",
    "        if not split_decision:\n",
    "            # Alphabetically, False comes first\n",
    "            dot_string += '    node{0} [label=\"x{1} = {2}?\"];\\n'.format(node_id, attribute_index, attribute_value)\n",
    "\n",
    "        if type(sub_trees) is dict:\n",
    "            if not split_decision:\n",
    "                dot_string, right_child, uid = to_graphviz(sub_trees, dot_string=dot_string, uid=uid, depth=depth + 1)\n",
    "                dot_string += '    node{0} -> node{1} [label=\"False\"];\\n'.format(node_id, right_child)\n",
    "            else:\n",
    "                dot_string, left_child, uid = to_graphviz(sub_trees, dot_string=dot_string, uid=uid, depth=depth + 1)\n",
    "                dot_string += '    node{0} -> node{1} [label=\"True\"];\\n'.format(node_id, left_child)\n",
    "\n",
    "        else:\n",
    "            uid += 1\n",
    "            dot_string += '    node{0} [label=\"y = {1}\"];\\n'.format(uid, sub_trees)\n",
    "            if not split_decision:\n",
    "                dot_string += '    node{0} -> node{1} [label=\"False\"];\\n'.format(node_id, uid)\n",
    "            else:\n",
    "                dot_string += '    node{0} -> node{1} [label=\"True\"];\\n'.format(node_id, uid)\n",
    "\n",
    "    if depth == 0:\n",
    "        dot_string += '}\\n'\n",
    "        return dot_string\n",
    "    else:\n",
    "        return dot_string, node_id, uid\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    y_pred_depth = {}\n",
    "    y_pred_d = {}\n",
    "    for d in range(1,6,2):\n",
    "        # Load the training data\n",
    "        M = np.genfromtxt('./breast-cancer.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "        y_train = M[:, -1]\n",
    "        x_train = M[:, :-1]\n",
    "        \n",
    "        # Load the test data\n",
    "        M = np.genfromtxt('./breast-cancer.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "        y_test = M[:, -1]\n",
    "        x_test = M[:, :-1]\n",
    "        \n",
    "        #Getting attribute_value_pairs\n",
    "        attribute_value_pairs = [] \n",
    "        for att in range(len(x_train[0])):\n",
    "            values_data = np.unique(x_train[:,att])\n",
    "            for v in range(len(values_data)):\n",
    "                attribute_value_pairs.append((att,values_data[v]))\n",
    "                \n",
    "        decision_tree = id3(x_train, y_train, attribute_value_pairs, max_depth=d)\n",
    "        \n",
    "        # Pretty print it to console\n",
    "        print('Printing decision tree from above written method for depth ' + str(d))\n",
    "        pretty_print(decision_tree)\n",
    "        \n",
    "        # Visualize the tree and save it as a PNG image\n",
    "        dot_str = to_graphviz(decision_tree)\n",
    "        render_dot_file(dot_str, './my_learned_tree_depth_E'+str(d))\n",
    "        \n",
    "        # Compute the test error\n",
    "        y_pred1 = [predict_example(x, decision_tree) for x in x_test]\n",
    "        y_pred_depth[d] = y_pred1\n",
    "        tst_err1 = compute_error(y_test, y_pred1)\n",
    "        print('Test Error = {0:4.2f}%.'.format(tst_err1 * 100))\n",
    "        \n",
    "        Tree = DecisionTreeClassifier(max_depth=d,criterion='entropy')\n",
    "        Tree.fit(x_train, y_train)\n",
    "        pretty_print1 = tree.export_text(Tree)\n",
    "        print('Printing decision tree by scikitlearns decision tree classifier for depth ' + str(d))\n",
    "        print(pretty_print1)\n",
    "        \n",
    "        y_pred2 = Tree.predict(x_test)\n",
    "        y_pred_d[d] = y_pred2\n",
    "        tst_err2 = compute_error(y_test, y_pred2)\n",
    "        print('Test Error = {0:4.2f}%.'.format(tst_err2 * 100))\n",
    "        \n",
    "from sklearn.metrics import confusion_matrix\n",
    "for d in range(1,6,2): \n",
    "    print(\"Confusion Matrix for depth \" + str(d) + ' for decision tree using our own method')\n",
    "    print(confusion_matrix(y_test, y_pred_depth.get(d)))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Confusion Matrix for depth \" + str(d) + ' for decision tree using scikit learn')\n",
    "    print(confusion_matrix(y_test, y_pred_d.get(d)))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ba8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
